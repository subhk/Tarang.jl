<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>GPU Computing · Tarang.jl</title><meta name="title" content="GPU Computing · Tarang.jl"/><meta property="og:title" content="GPU Computing · Tarang.jl"/><meta property="twitter:title" content="GPU Computing · Tarang.jl"/><meta name="description" content="Documentation for Tarang.jl."/><meta property="og:description" content="Documentation for Tarang.jl."/><meta property="twitter:description" content="Documentation for Tarang.jl."/><meta property="og:url" content="https://subhk.github.io/Tarang.jl/pages/gpu_computing/"/><meta property="twitter:url" content="https://subhk.github.io/Tarang.jl/pages/gpu_computing/"/><link rel="canonical" href="https://subhk.github.io/Tarang.jl/pages/gpu_computing/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/custom.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="Tarang.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Tarang.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><input class="collapse-toggle" id="menuitem-2" type="checkbox"/><label class="tocitem" for="menuitem-2"><span class="docs-label">Installing Tarang</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../getting_started/installation/">Installation</a></li><li><a class="tocitem" href="../../getting_started/first_steps/">First Steps with Tarang.jl</a></li><li><a class="tocitem" href="../../getting_started/running_with_mpi/">Running with MPI</a></li><li><a class="tocitem" href="../configuration/">Configuration</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Tutorials &amp; Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">Tutorial Notebooks</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../tutorials/overview/">Tutorials Overview</a></li><li><a class="tocitem" href="../../tutorials/ivp_2d_rbc/">Tutorial: 2D Rayleigh-Bénard Convection</a></li><li><a class="tocitem" href="../../tutorials/ivp_3d_turbulence/">Tutorial: 3D Turbulence Simulation</a></li><li><a class="tocitem" href="../../tutorials/boundary_conditions/">Tutorial: Boundary Conditions</a></li><li><a class="tocitem" href="../../tutorials/analysis_and_output/">Tutorial: Analysis and Output</a></li><li><a class="tocitem" href="../../tutorials/eigenvalue_problems/">Tutorial: Eigenvalue Problems</a></li><li><a class="tocitem" href="../../tutorials/surface_dynamics/">Surface and Boundary Dynamics</a></li><li><a class="tocitem" href="../../tutorials/rotating_shallow_water/">Rotating Shallow Water with Lagrangian Averaging</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-2" type="checkbox"/><label class="tocitem" for="menuitem-3-2"><span class="docs-label">Example Scripts</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../examples/gallery/">Example Gallery</a></li><li><a class="tocitem" href="../../examples/fluid_dynamics/">Fluid Dynamics Examples</a></li><li><a class="tocitem" href="../../examples/heat_transfer/">Heat Transfer Examples</a></li><li><a class="tocitem" href="../../examples/eigenvalue_analysis/">Eigenvalue Analysis Examples</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Jupyter Notebooks</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../notebooks/rayleigh_benard/">Notebook: Rayleigh-Bénard Convection</a></li><li><a class="tocitem" href="../../notebooks/channel_flow/">Notebook: Channel Flow</a></li><li><a class="tocitem" href="../../notebooks/taylor_green/">Notebook: Taylor-Green Vortex</a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox" checked/><label class="tocitem" for="menuitem-4"><span class="docs-label">User Guide</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-4-1" type="checkbox"/><label class="tocitem" for="menuitem-4-1"><span class="docs-label">Core Concepts</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../coordinates/">Coordinates</a></li><li><a class="tocitem" href="../bases/">Spectral Bases</a></li><li><a class="tocitem" href="../domains/">Domains</a></li><li><a class="tocitem" href="../fields/">Fields</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-2" type="checkbox"/><label class="tocitem" for="menuitem-4-2"><span class="docs-label">Problem Setup</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../operators/">Operators</a></li><li><a class="tocitem" href="../problems/">Problems</a></li><li><a class="tocitem" href="../solvers/">Solvers</a></li><li><a class="tocitem" href="../timesteppers/">Time Steppers</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Physics &amp; Modeling</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../stochastic_forcing/">Stochastic Forcing</a></li><li><a class="tocitem" href="../temporal_filters/">Temporal Filters for Lagrangian Averaging</a></li><li><a class="tocitem" href="../les_models/">Large Eddy Simulation (LES) Models</a></li><li><a class="tocitem" href="../gql_approximation/">Generalized Quasi-Linear (GQL) Approximation</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-4" type="checkbox" checked/><label class="tocitem" for="menuitem-4-4"><span class="docs-label">Performance &amp; Parallelism</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>GPU Computing</a><ul class="internal"><li><a class="tocitem" href="#Overview"><span>Overview</span></a></li><li><a class="tocitem" href="#Quick-Start"><span>Quick Start</span></a></li><li><a class="tocitem" href="#GPU-Transforms"><span>GPU Transforms</span></a></li><li><a class="tocitem" href="#GPU-Memory-Management"><span>GPU Memory Management</span></a></li><li><a class="tocitem" href="#Custom-GPU-Kernels"><span>Custom GPU Kernels</span></a></li><li><a class="tocitem" href="#Distributed-GPU-Computing"><span>Distributed GPU Computing</span></a></li><li><a class="tocitem" href="#Performance-Optimization"><span>Performance Optimization</span></a></li><li><a class="tocitem" href="#Tensor-Core-Support"><span>Tensor Core Support</span></a></li><li><a class="tocitem" href="#Troubleshooting"><span>Troubleshooting</span></a></li><li><a class="tocitem" href="#API-Reference"><span>API Reference</span></a></li></ul></li><li><a class="tocitem" href="../parallelism/">Parallelism</a></li><li><a class="tocitem" href="../optimization/">Performance Optimization</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-5" type="checkbox"/><label class="tocitem" for="menuitem-4-5"><span class="docs-label">Analysis &amp; Output</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../analysis/">Analysis</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-6" type="checkbox"/><label class="tocitem" for="menuitem-4-6"><span class="docs-label">Advanced Topics</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../tau_method/">The Tau Method for Boundary Conditions</a></li><li><a class="tocitem" href="../custom_operators/">Custom Operators</a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-5" type="checkbox"/><label class="tocitem" for="menuitem-5"><span class="docs-label">API Reference</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><input class="collapse-toggle" id="menuitem-5-1" type="checkbox"/><label class="tocitem" for="menuitem-5-1"><span class="docs-label">Core</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../api/coordinates/">Coordinates API</a></li><li><a class="tocitem" href="../../api/bases/">Bases API</a></li><li><a class="tocitem" href="../../api/domains/">Domains API</a></li><li><a class="tocitem" href="../../api/fields/">Fields API</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-2" type="checkbox"/><label class="tocitem" for="menuitem-5-2"><span class="docs-label">Operators &amp; Problems</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../api/operators/">Operators API</a></li><li><a class="tocitem" href="../../api/problems/">Problems API</a></li><li><a class="tocitem" href="../../api/solvers/">Solvers API</a></li><li><a class="tocitem" href="../../api/timesteppers/">Timesteppers API</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-3" type="checkbox"/><label class="tocitem" for="menuitem-5-3"><span class="docs-label">GPU &amp; Performance</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../api/gpu/">GPU API Reference</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-5-4" type="checkbox"/><label class="tocitem" for="menuitem-5-4"><span class="docs-label">Extras</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../api/stochastic_forcing/">Stochastic Forcing API</a></li><li><a class="tocitem" href="../../api/les_models/">LES Models API</a></li><li><a class="tocitem" href="../../api/analysis/">Analysis API</a></li><li><a class="tocitem" href="../../api/io/">I/O API</a></li></ul></li></ul></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">Development</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../contributing/">Contributing</a></li><li><a class="tocitem" href="../architecture/">Architecture</a></li><li><a class="tocitem" href="../testing/">Testing</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">User Guide</a></li><li><a class="is-disabled">Performance &amp; Parallelism</a></li><li class="is-active"><a href>GPU Computing</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>GPU Computing</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/subhk/Tarang.jl/blob/main/docs/src/pages/gpu_computing.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="GPU-Computing"><a class="docs-heading-anchor" href="#GPU-Computing">GPU Computing</a><a id="GPU-Computing-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Computing" title="Permalink"></a></h1><p>Tarang.jl provides comprehensive GPU acceleration through CUDA.jl, enabling significant speedups for spectral simulations on NVIDIA GPUs.</p><h2 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h2><div class="admonition is-info">
<p class="admonition-title">Requirements</p>
<p>GPU support requires an NVIDIA GPU with CUDA capability 5.0+ and the CUDA.jl package.</p>
</div><h3 id="Key-Features"><a class="docs-heading-anchor" href="#Key-Features">Key Features</a><a id="Key-Features-1"></a><a class="docs-heading-anchor-permalink" href="#Key-Features" title="Permalink"></a></h3><table><tr><th style="text-align: right">Feature</th><th style="text-align: right">Description</th></tr><tr><td style="text-align: right"><strong>Automatic Dispatch</strong></td><td style="text-align: right">Arrays automatically use GPU kernels when on GPU memory</td></tr><tr><td style="text-align: right"><strong>CUFFT Integration</strong></td><td style="text-align: right">Optimized FFT plans via NVIDIA&#39;s cuFFT library</td></tr><tr><td style="text-align: right"><strong>Custom Kernels</strong></td><td style="text-align: right">KernelAbstractions.jl for portable CPU/GPU code</td></tr><tr><td style="text-align: right"><strong>Memory Pools</strong></td><td style="text-align: right">Efficient GPU memory management with pooling</td></tr><tr><td style="text-align: right"><strong>Multi-GPU</strong></td><td style="text-align: right">MPI + CUDA for distributed GPU computing</td></tr><tr><td style="text-align: right"><strong>Mixed Precision</strong></td><td style="text-align: right">Float32 support for memory-bound problems</td></tr></table><h2 id="Quick-Start"><a class="docs-heading-anchor" href="#Quick-Start">Quick Start</a><a id="Quick-Start-1"></a><a class="docs-heading-anchor-permalink" href="#Quick-Start" title="Permalink"></a></h2><h3 id="Basic-GPU-Setup"><a class="docs-heading-anchor" href="#Basic-GPU-Setup">Basic GPU Setup</a><a id="Basic-GPU-Setup-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-GPU-Setup" title="Permalink"></a></h3><pre><code class="language-julia hljs">using Tarang
using CUDA

# Check GPU availability
@assert CUDA.functional() &quot;CUDA not available&quot;

# Create distributor with GPU architecture
coords = CartesianCoordinates(&quot;x&quot;, &quot;y&quot;)
dist = Distributor(coords; mesh=(1,), dtype=Float64, architecture=GPU())

# Create bases and fields (automatically on GPU)
xbasis = Fourier(coords, &quot;x&quot;, 256)
ybasis = Fourier(coords, &quot;y&quot;, 256)
field = ScalarField(dist, &quot;u&quot;, (xbasis, ybasis))

# Field data is a CuArray
@assert field[&quot;g&quot;] isa CuArray</code></pre><h3 id="CPU-vs-GPU-Architecture"><a class="docs-heading-anchor" href="#CPU-vs-GPU-Architecture">CPU vs GPU Architecture</a><a id="CPU-vs-GPU-Architecture-1"></a><a class="docs-heading-anchor-permalink" href="#CPU-vs-GPU-Architecture" title="Permalink"></a></h3><pre><code class="language-julia hljs"># CPU execution (default)
dist_cpu = Distributor(coords; architecture=CPU())

# GPU execution
dist_gpu = Distributor(coords; architecture=GPU())

# Check architecture
arch = dist_gpu.architecture  # GPU()</code></pre><h2 id="GPU-Transforms"><a class="docs-heading-anchor" href="#GPU-Transforms">GPU Transforms</a><a id="GPU-Transforms-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Transforms" title="Permalink"></a></h2><h3 id="Automatic-FFT-Acceleration"><a class="docs-heading-anchor" href="#Automatic-FFT-Acceleration">Automatic FFT Acceleration</a><a id="Automatic-FFT-Acceleration-1"></a><a class="docs-heading-anchor-permalink" href="#Automatic-FFT-Acceleration" title="Permalink"></a></h3><p>When fields are on GPU, transforms automatically use CUFFT:</p><pre><code class="language-julia hljs">using Tarang, CUDA

dist = Distributor(coords; architecture=GPU())
field = ScalarField(dist, &quot;u&quot;, (xbasis, ybasis))

# Initialize with GPU data
field[&quot;g&quot;] .= CUDA.rand(Float64, size(field[&quot;g&quot;])...)

# Forward transform (uses CUFFT automatically)
forward_transform!(field)

# Backward transform
backward_transform!(field)</code></pre><h3 id="FFT-Mode-Control"><a class="docs-heading-anchor" href="#FFT-Mode-Control">FFT Mode Control</a><a id="FFT-Mode-Control-1"></a><a class="docs-heading-anchor-permalink" href="#FFT-Mode-Control" title="Permalink"></a></h3><p>Control when GPU FFTs are used:</p><pre><code class="language-julia hljs"># Per-field control
set_gpu_fft_mode!(field, :gpu)   # Always use GPU FFT
set_gpu_fft_mode!(field, :cpu)   # Always use CPU FFT
set_gpu_fft_mode!(field, :auto)  # Heuristic-based (default)

# Global threshold for :auto mode
# Use GPU FFT only if array has &gt;= N elements
set_gpu_fft_min_elements!(64_000)</code></pre><h3 id="Mixed-Fourier-Chebyshev-Transforms"><a class="docs-heading-anchor" href="#Mixed-Fourier-Chebyshev-Transforms">Mixed Fourier-Chebyshev Transforms</a><a id="Mixed-Fourier-Chebyshev-Transforms-1"></a><a class="docs-heading-anchor-permalink" href="#Mixed-Fourier-Chebyshev-Transforms" title="Permalink"></a></h3><p>GPU DCT for Chebyshev bases:</p><pre><code class="language-julia hljs"># Mixed basis domain
xbasis = Fourier(coords, &quot;x&quot;, 256)      # FFT
zbasis = ChebyshevT(coords, &quot;z&quot;, 64)    # DCT

dist = Distributor(coords; architecture=GPU())
field = ScalarField(dist, &quot;T&quot;, (xbasis, zbasis))

# Transforms automatically select FFT or DCT per dimension
forward_transform!(field)   # FFT in x, DCT in z
backward_transform!(field)  # IFFT in x, IDCT in z</code></pre><h2 id="GPU-Memory-Management"><a class="docs-heading-anchor" href="#GPU-Memory-Management">GPU Memory Management</a><a id="GPU-Memory-Management-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Memory-Management" title="Permalink"></a></h2><h3 id="Memory-Pools"><a class="docs-heading-anchor" href="#Memory-Pools">Memory Pools</a><a id="Memory-Pools-1"></a><a class="docs-heading-anchor-permalink" href="#Memory-Pools" title="Permalink"></a></h3><p>Tarang uses memory pooling to reduce allocation overhead:</p><pre><code class="language-julia hljs">using TarangCUDAExt: GPUMemoryPool, pool_allocate, pool_release!

# Get memory pool statistics
stats = memory_pool_stats()
println(&quot;Allocated: $(stats.allocated_bytes) bytes&quot;)
println(&quot;Cached: $(stats.cached_bytes) bytes&quot;)

# Clear the memory pool (frees cached memory)
clear_memory_pool!()</code></pre><h3 id="Pinned-Memory-for-MPI"><a class="docs-heading-anchor" href="#Pinned-Memory-for-MPI">Pinned Memory for MPI</a><a id="Pinned-Memory-for-MPI-1"></a><a class="docs-heading-anchor-permalink" href="#Pinned-Memory-for-MPI" title="Permalink"></a></h3><p>For efficient GPU-MPI transfers:</p><pre><code class="language-julia hljs">using TarangCUDAExt: get_pinned_buffer, async_copy_to_gpu!, async_copy_to_cpu!

# Get a pinned CPU buffer for async transfers
buffer = get_pinned_buffer(Float64, 1024)

# Async copy operations
async_copy_to_gpu!(gpu_array, buffer)
async_copy_to_cpu!(buffer, gpu_array)</code></pre><h3 id="Memory-Monitoring"><a class="docs-heading-anchor" href="#Memory-Monitoring">Memory Monitoring</a><a id="Memory-Monitoring-1"></a><a class="docs-heading-anchor-permalink" href="#Memory-Monitoring" title="Permalink"></a></h3><pre><code class="language-julia hljs">using TarangCUDAExt: gpu_memory_info, check_gpu_memory

# Get current memory usage
info = gpu_memory_info()
println(&quot;Free: $(info.free_bytes / 1e9) GB&quot;)
println(&quot;Total: $(info.total_bytes / 1e9) GB&quot;)

# Check if allocation will fit
can_allocate = check_gpu_memory(required_bytes)</code></pre><h2 id="Custom-GPU-Kernels"><a class="docs-heading-anchor" href="#Custom-GPU-Kernels">Custom GPU Kernels</a><a id="Custom-GPU-Kernels-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-GPU-Kernels" title="Permalink"></a></h2><h3 id="KernelAbstractions-Integration"><a class="docs-heading-anchor" href="#KernelAbstractions-Integration">KernelAbstractions Integration</a><a id="KernelAbstractions-Integration-1"></a><a class="docs-heading-anchor-permalink" href="#KernelAbstractions-Integration" title="Permalink"></a></h3><p>Write portable kernels that run on both CPU and GPU:</p><pre><code class="language-julia hljs">using Tarang, KernelAbstractions

# Define a kernel
@kernel function add_kernel!(c, @Const(a), @Const(b))
    i = @index(Global)
    @inbounds c[i] = a[i] + b[i]
end

# Wrap as KernelOperation
add_op = KernelOperation(add_kernel!) do c, a, b
    length(c)  # ndrange
end

# Use on any architecture
arch = GPU()
a = ones(arch, Float64, 1024)
b = ones(arch, Float64, 1024)
c = zeros(arch, Float64, 1024)

add_op(arch, c, a, b)  # Runs on GPU</code></pre><h3 id="Built-in-GPU-Kernels"><a class="docs-heading-anchor" href="#Built-in-GPU-Kernels">Built-in GPU Kernels</a><a id="Built-in-GPU-Kernels-1"></a><a class="docs-heading-anchor-permalink" href="#Built-in-GPU-Kernels" title="Permalink"></a></h3><p>Tarang provides optimized kernels for common operations:</p><pre><code class="language-julia hljs">using TarangCUDAExt

# Element-wise operations
gpu_add!(c, a, b)           # c = a + b
gpu_sub!(c, a, b)           # c = a - b
gpu_mul!(c, a, b)           # c = a * b
gpu_scale!(y, α, x)         # y = α * x
gpu_axpy!(y, α, x)          # y = y + α * x
gpu_linear_combination!(y, α, a, β, b)  # y = α*a + β*b

# Fused operations for timestepping
gpu_rk_stage!(u_new, u, k, dt, coeff)
gpu_axpby!(y, α, x, β)      # y = α*x + β*y

# Physics kernels
gpu_kinetic_energy_2d!(ke, ux, uy)
gpu_kinetic_energy_3d!(ke, ux, uy, uz)
gpu_viscous_damping!(field, ν, k2)</code></pre><h2 id="Distributed-GPU-Computing"><a class="docs-heading-anchor" href="#Distributed-GPU-Computing">Distributed GPU Computing</a><a id="Distributed-GPU-Computing-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-GPU-Computing" title="Permalink"></a></h2><h3 id="MPI-CUDA"><a class="docs-heading-anchor" href="#MPI-CUDA">MPI + CUDA</a><a id="MPI-CUDA-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-CUDA" title="Permalink"></a></h3><p>For multi-GPU simulations:</p><pre><code class="language-julia hljs">using Tarang, MPI, CUDA

MPI.Init()

comm = MPI.COMM_WORLD
rank = MPI.Comm_rank(comm)

# Assign GPU to MPI rank
if CUDA.ndevices() &gt;= MPI.Comm_size(comm)
    CUDA.device!(rank)
end

# Create distributed GPU setup
coords = CartesianCoordinates(&quot;x&quot;, &quot;y&quot;, &quot;z&quot;)
dist = Distributor(coords; mesh=(2, 2), architecture=GPU())

# Each rank has its own GPU memory
field = ScalarField(dist, &quot;u&quot;, bases)</code></pre><h3 id="CUDA-Aware-MPI"><a class="docs-heading-anchor" href="#CUDA-Aware-MPI">CUDA-Aware MPI</a><a id="CUDA-Aware-MPI-1"></a><a class="docs-heading-anchor-permalink" href="#CUDA-Aware-MPI" title="Permalink"></a></h3><p>For direct GPU-to-GPU communication:</p><pre><code class="language-julia hljs">using TarangCUDAExt: check_cuda_aware_mpi

if check_cuda_aware_mpi()
    println(&quot;CUDA-aware MPI available - using direct GPU transfers&quot;)
else
    println(&quot;Staging through CPU for MPI transfers&quot;)
end</code></pre><h3 id="TransposableField-for-GPUMPI"><a class="docs-heading-anchor" href="#TransposableField-for-GPUMPI">TransposableField for GPU+MPI</a><a id="TransposableField-for-GPUMPI-1"></a><a class="docs-heading-anchor-permalink" href="#TransposableField-for-GPUMPI" title="Permalink"></a></h3><p>Efficient distributed FFTs with GPU:</p><pre><code class="language-julia hljs"># Create transposable field for distributed transforms
field = ScalarField(dist, &quot;u&quot;, bases)
tf = TransposableField(field)

# Distributed forward transform (handles GPU transposes)
distributed_forward_transform!(tf)

# Distributed backward transform
distributed_backward_transform!(tf)</code></pre><h2 id="Performance-Optimization"><a class="docs-heading-anchor" href="#Performance-Optimization">Performance Optimization</a><a id="Performance-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Performance-Optimization" title="Permalink"></a></h2><h3 id="Best-Practices"><a class="docs-heading-anchor" href="#Best-Practices">Best Practices</a><a id="Best-Practices-1"></a><a class="docs-heading-anchor-permalink" href="#Best-Practices" title="Permalink"></a></h3><ol><li><p><strong>Use Float32 when possible</strong> - 2x memory bandwidth, often sufficient accuracy</p><pre><code class="language-julia hljs">dist = Distributor(coords; dtype=Float32, architecture=GPU())</code></pre></li><li><p><strong>Batch operations</strong> - Minimize kernel launches</p><pre><code class="language-julia hljs"># Bad: many small operations
for i in 1:n
    gpu_scale!(field, α)
end

# Good: fused operations
gpu_rk_stage!(u_new, u, k, dt, coeff)</code></pre></li><li><p><strong>Preallocate buffers</strong> - Avoid allocation in hot loops</p><pre><code class="language-julia hljs"># Preallocate work arrays
work = zeros(GPU(), Float64, size(field[&quot;g&quot;]))

for step in 1:nsteps
    # Reuse work array
    compute!(work, field)
end</code></pre></li><li><p><strong>Use streams for overlap</strong> - Overlap computation and communication</p><pre><code class="language-julia hljs">using TarangCUDAExt: get_compute_stream, get_transfer_stream, sync_streams!

compute_stream = get_compute_stream()
transfer_stream = get_transfer_stream()
# ... overlap operations
sync_streams!()</code></pre></li></ol><h3 id="Profiling"><a class="docs-heading-anchor" href="#Profiling">Profiling</a><a id="Profiling-1"></a><a class="docs-heading-anchor-permalink" href="#Profiling" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CUDA

# Profile a section
CUDA.@profile begin
    forward_transform!(field)
    backward_transform!(field)
end

# Time with synchronization
CUDA.@elapsed begin
    forward_transform!(field)
    CUDA.synchronize()
end</code></pre><h3 id="Memory-Bandwidth-Optimization"><a class="docs-heading-anchor" href="#Memory-Bandwidth-Optimization">Memory Bandwidth Optimization</a><a id="Memory-Bandwidth-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Memory-Bandwidth-Optimization" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Check if transform is memory-bound
n = prod(size(field[&quot;g&quot;]))
bytes_transferred = n * sizeof(eltype(field[&quot;g&quot;])) * 4  # rough estimate

# Theoretical bandwidth (e.g., A100 = 2 TB/s)
theoretical_time = bytes_transferred / 2e12

# Compare to actual time
actual_time = CUDA.@elapsed forward_transform!(field)
efficiency = theoretical_time / actual_time
println(&quot;Memory bandwidth efficiency: $(efficiency * 100)%&quot;)</code></pre><h2 id="Tensor-Core-Support"><a class="docs-heading-anchor" href="#Tensor-Core-Support">Tensor Core Support</a><a id="Tensor-Core-Support-1"></a><a class="docs-heading-anchor-permalink" href="#Tensor-Core-Support" title="Permalink"></a></h2><p>For supported operations on Ampere+ GPUs:</p><pre><code class="language-julia hljs">using TarangCUDAExt: enable_tensor_cores!, disable_tensor_cores!

# Enable tensor cores (requires compatible data types)
enable_tensor_cores!()

# Disable if numerical precision is critical
disable_tensor_cores!()</code></pre><h2 id="Troubleshooting"><a class="docs-heading-anchor" href="#Troubleshooting">Troubleshooting</a><a id="Troubleshooting-1"></a><a class="docs-heading-anchor-permalink" href="#Troubleshooting" title="Permalink"></a></h2><h3 id="Common-Issues"><a class="docs-heading-anchor" href="#Common-Issues">Common Issues</a><a id="Common-Issues-1"></a><a class="docs-heading-anchor-permalink" href="#Common-Issues" title="Permalink"></a></h3><p><strong>Out of Memory</strong></p><pre><code class="language-julia hljs"># Check available memory before large allocations
info = gpu_memory_info()
if info.free_bytes &lt; required_bytes
    clear_memory_pool!()  # Free cached allocations
    GC.gc()               # Trigger garbage collection
    CUDA.reclaim()        # Reclaim CUDA memory
end</code></pre><p><strong>Slow Performance</strong></p><pre><code class="language-julia hljs"># Ensure synchronization isn&#39;t killing performance
CUDA.allowscalar(false)  # Disable slow scalar indexing

# Check for CPU fallbacks
@assert field[&quot;g&quot;] isa CuArray &quot;Data not on GPU!&quot;</code></pre><p><strong>MPI + CUDA Issues</strong></p><pre><code class="language-julia hljs"># Ensure correct GPU assignment
println(&quot;Rank $rank using GPU $(CUDA.device())&quot;)

# Force synchronization before MPI calls
CUDA.synchronize()
MPI.Barrier(comm)</code></pre><h2 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h2><p>See the <a href="../../api/gpu/">GPU API documentation</a> for complete function references.</p><h3 id="Key-Functions"><a class="docs-heading-anchor" href="#Key-Functions">Key Functions</a><a id="Key-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Key-Functions" title="Permalink"></a></h3><table><tr><th style="text-align: right">Function</th><th style="text-align: right">Description</th></tr><tr><td style="text-align: right"><code>GPU()</code></td><td style="text-align: right">GPU architecture singleton</td></tr><tr><td style="text-align: right"><code>on_architecture(GPU(), array)</code></td><td style="text-align: right">Move array to GPU</td></tr><tr><td style="text-align: right"><code>forward_transform!(field)</code></td><td style="text-align: right">GPU-accelerated forward FFT</td></tr><tr><td style="text-align: right"><code>backward_transform!(field)</code></td><td style="text-align: right">GPU-accelerated inverse FFT</td></tr><tr><td style="text-align: right"><code>set_gpu_fft_mode!(field, mode)</code></td><td style="text-align: right">Control FFT backend</td></tr><tr><td style="text-align: right"><code>TransposableField(field)</code></td><td style="text-align: right">Distributed GPU transforms</td></tr></table></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../gql_approximation/">« Generalized Quasi-Linear (GQL) Approximation</a><a class="docs-footer-nextpage" href="../parallelism/">Parallelism »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and <a href="https://julialang.org">Julia</a>. Tarang.jl © 2024 Subhajit Kar.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 23 January 2026 22:04">Friday 23 January 2026</span>. Using Julia version 1.10.10.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
